{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c27f0942",
   "metadata": {},
   "source": [
    "# PCAM Dataset Extractor - Quick Start Guide\n",
    "\n",
    "This script extracts and downsamples the PatchCamelyon (PCAM) dataset from HDF5 files while maintaining class balance through stratified sampling. The extractor reads labels first, randomly selects indices proportional to each class distribution, then extracts only those selected images—making it highly memory-efficient even for large datasets.\n",
    "\n",
    "To create a custom subset, modify three key values in the `if __name__ == \"__main__\":` section at the bottom:\n",
    "- **`BASE_DIR`**: Set to your H5 files location (e.g., `Path(r\"E:\\PCam\")`)\n",
    "- **`OUTPUT_DIR`**: Set to where you want extracted images saved (e.g., `Path(r\"E:\\PCam_Extracted_50k\")`)\n",
    "- **`TARGET_SIZE`**: Set to your desired total dataset size (e.g., `50000` for 50k images, `10000` for 10k images)\n",
    "\n",
    "The script automatically splits your target size into 80% train, 10% validation, and 10% test sets while preserving the original class distribution in each split. The `RANDOM_SEED = 42` ensures reproducibility—using the same seed always selects the same images, allowing you to verify results or share exact subsets with collaborators.\n",
    "\n",
    "**Note:** Originally created to run in VS Code, not Google Colab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71be52d7",
   "metadata": {},
   "source": [
    "100K Images Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2abb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py # Reading HDF5 (.h5) file format\n",
    "import numpy as np # Numerical operations and array manipulation\n",
    "from pathlib import Path # Cross-platform file path handling\n",
    "from PIL import Image # Image processing and saving\n",
    "from tqdm import tqdm # Displayinng progress bars during loops\n",
    "\n",
    "def calculate_split_sizes(total_target=100000, train_ratio=0.8, val_ratio=0.1, test_ratio=0.1):\n",
    "    \"\"\"\n",
    "    Purpose:\n",
    "    --------\n",
    "    Calculate how many images to keep in each split.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    total_target : int\n",
    "        Total number of images desired in downsampled dataset\n",
    "    train_ratio, val_ratio, test_ratio : float\n",
    "        Ratios for train/val/test split\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Number of images to keep for each split\n",
    "    \"\"\"\n",
    "    # Calculate number of training images by multiplying total by train ratio\n",
    "    train_size = int(total_target * train_ratio)\n",
    "    # Calculate number of validation images by multiplying total by validation ratio\n",
    "    val_size = int(total_target * val_ratio)\n",
    "    # Calculate number of test images by multiplying by test ratio\n",
    "    test_size = int(total_target * test_ratio)\n",
    "    \n",
    "    # Adjust to ensure exact total\n",
    "    # Calcualte sum of all three split sizes to check for rounding errors\n",
    "    actual_total = train_size + val_size + test_size\n",
    "    # Check if actual total differs from target due to integer rounding\n",
    "    if actual_total != total_target:\n",
    "        # Add the difference to the training set to reach the exact target total\n",
    "        train_size += (total_target - actual_total)\n",
    "    \n",
    "    # Return a dictionary mapping split names to their calculated sizes\n",
    "    return {\n",
    "        'train': train_size,\n",
    "        'valid': val_size,\n",
    "        'test': test_size\n",
    "    }\n",
    "\n",
    "\n",
    "def get_stratified_indices(labels, n_samples, random_seed=42):\n",
    "    \"\"\"\n",
    "    Purpose:\n",
    "    --------\n",
    "    Get stratified sample indices to maintain class balance.\n",
    "    Uses a fixed seed for reproducibility.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    labels : numpy.ndarray\n",
    "        Array of binary labels (0 or 1)\n",
    "    n_samples : int\n",
    "        Number of samples to select\n",
    "    random_seed : int\n",
    "        Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    selected_indices : numpy.ndarray\n",
    "        Sorted array of selected indices\n",
    "    \"\"\"\n",
    "    # Set random seed for reproducible sampling across runs\n",
    "    # Initializes the psuedo-random number generator (PRNG) with a specific starting point\n",
    "    np.random.seed(random_seed)\n",
    "    \n",
    "    # Get indices for each class\n",
    "    # Find all indices where the label is 0 (no tumor class)\n",
    "    class_0_indices = np.where(labels == 0)[0]\n",
    "    # Find all indices where the label is 1 (tumor class)\n",
    "    class_1_indices = np.where(labels == 1)[0]\n",
    "    \n",
    "    # Calculate how many of each class to keep (proportional)\n",
    "    # Count total number of class 0 samples\n",
    "    total_class_0 = len(class_0_indices)\n",
    "    # Count total number of class 1 samples\n",
    "    total_class_1 = len(class_1_indices)\n",
    "    # Calculate total number of sampels across both classes\n",
    "    total = total_class_0 + total_class_1\n",
    "    \n",
    "    # Calculate how many class 0 samples to keep, proportional to original distribution\n",
    "    n_class_0 = int(n_samples * (total_class_0 / total))\n",
    "    # Calculate class 1 samples as remainder to ensure exact n_samples total\n",
    "    n_class_1 = n_samples - n_class_0\n",
    "    \n",
    "    # Ensure we don't try to sample more than available\n",
    "    # Sample the smallest amount (either the number of class 0 claculated or the total number of class 0 images)\n",
    "    n_class_0 = min(n_class_0, total_class_0)\n",
    "    # Sample the smallest amount (either the number of class 1 claculated or the total number of class 01images)\n",
    "    n_class_1 = min(n_class_1, total_class_1)\n",
    "    \n",
    "    # Randomly sample from each class (but reproducibly due to the set seed)\n",
    "    # Randomly select n_class_0 indices from class 0 without replacement\n",
    "    selected_class_0 = np.random.choice(class_0_indices, size=n_class_0, replace=False)\n",
    "    # Randomly select n_class_1 indices from class 1 without replacement\n",
    "    selected_class_1 = np.random.choice(class_1_indices, size=n_class_1, replace=False)\n",
    "    \n",
    "    # Combine and sort indices\n",
    "    # Combine the selected indices from both classes into a single array\n",
    "    selected_indices = np.concatenate([selected_class_0, selected_class_1])\n",
    "    # Sort the indices in ascending order for sequential access\n",
    "    selected_indices = np.sort(selected_indices)\n",
    "    \n",
    "    return selected_indices # Return the sorted array of selected indices\n",
    "\n",
    "\n",
    "def extract_and_downsample_split(images_h5_path, labels_h5_path, output_dir, \n",
    "                                  split_name, n_samples, random_seed=42, \n",
    "                                  save_format='png'):\n",
    "    \"\"\"\n",
    "    Purpose:\n",
    "    --------\n",
    "    Extract and downsample a single split directly from H5 files.\n",
    "    Only extracts the selected subset, never storing all images.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    images_h5_path : str or Path\n",
    "        Path to input images .h5 file\n",
    "    labels_h5_path : str or Path\n",
    "        Path to input labels .h5 file\n",
    "    output_dir : str or Path\n",
    "        Base directory where extracted images will be saved\n",
    "    split_name : str\n",
    "        Name of the split ('train', 'valid', 'test')\n",
    "    n_samples : int\n",
    "        Number of samples to extract\n",
    "    random_seed : int\n",
    "        Random seed for reproducibility\n",
    "    save_format : str\n",
    "        Image format to save ('png', 'jpg', etc.)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    selected_indices : numpy.ndarray\n",
    "        Selected indices (for verification)\n",
    "    \"\"\"\n",
    "    # Ensure the path to the images is a Path object\n",
    "    images_h5_path = Path(images_h5_path)\n",
    "    # Ensure the path to the labels is a Path object\n",
    "    labels_h5_path = Path(labels_h5_path)\n",
    "    # Convert the output directory (string or Path) to a Path object\n",
    "    output_dir = Path(output_dir)\n",
    "    \n",
    "    # Create split-specific directories: output_dir/train, output_dir/val, output_dir/test\n",
    "    split_dir = output_dir / split_name\n",
    "    # Create path for images subdirectory within the split\n",
    "    images_output_dir = split_dir / \"images\"\n",
    "    # Create path for labels subdirectory within the split\n",
    "    labels_output_dir = split_dir / \"labels\"\n",
    "    # Create images directory and all parent directories if they don't already exist\n",
    "    images_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    # Create labels directory and all parent directories if they don't already exist\n",
    "    labels_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    print(f\"\\n{'='*70}\") # Print separator line\n",
    "    # Print header showing which split is being processed\n",
    "    print(f\"Processing {split_name.upper()} split\")\n",
    "    print(f\"{'='*70}\") # Print separator line\n",
    "    # Print the source h5 filename\n",
    "    print(f\"Source: {images_h5_path.name}\")\n",
    "    # Print the target number of samples to extract\n",
    "    print(f\"Target samples: {n_samples}\")\n",
    "    \n",
    "    # Step 1: Load labels to determine stratified sampling\n",
    "    print(\"\\nStep 1: Loading labels for stratification...\")\n",
    "\n",
    "    try: # Try to find actual h5 file (manages both file nad directory paths)\n",
    "        # Call helper function to find the h5 labels file\n",
    "        actual_labels_path = find_h5_file(labels_h5_path)\n",
    "        # Print the resolved path to the labels file\n",
    "        print(f\"  Found labels H5: {actual_labels_path}\")\n",
    "    # Catch any file not found errors\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"  ERROR: {e}\") # Print error message\n",
    "        raise # Re-raise the exception to stop execution\n",
    "    # Open the labels h5 file in read mode\n",
    "    with h5py.File(actual_labels_path, 'r') as h5_file:\n",
    "        # Read all labels from the 'y' dataset in the h5 file\n",
    "        labels = h5_file['y'][:]\n",
    "        # Remove extra dimensions (converting shape (n,1) to (n,))\n",
    "        labels = labels.squeeze()\n",
    "    \n",
    "    # Get the total number of labels (original dataset size)\n",
    "    original_size = len(labels)\n",
    "    # Count how many samples have label 0 (no tumor)\n",
    "    class_0_count = np.sum(labels == 0)\n",
    "    # Count how many samples have label 1 (tumor)\n",
    "    class_1_count = np.sum(labels == 1)\n",
    "    \n",
    "    # Print original dataset size with thousands separator\n",
    "    print(f\"  Original size: {original_size:,}\")\n",
    "    # Print class 0 count and percentage\n",
    "    print(f\"  Class 0 (no tumor): {class_0_count:,} ({class_0_count/original_size*100:.1f}%)\")\n",
    "    # Print class 1 count and percentage\n",
    "    print(f\"  Class 1 (tumor):    {class_1_count:,} ({class_1_count/original_size*100:.1f}%)\")\n",
    "    \n",
    "    # Step 2: Get stratified sample indices\n",
    "    # Print step 2 header with the random seed value used\n",
    "    print(\"\\nStep 2: Selecting stratified sample (reproducible with seed={})...\".format(random_seed))\n",
    "    # Call function to get stratified sample indices maintaining class balance\n",
    "    selected_indices = get_stratified_indices(labels, n_samples, random_seed)\n",
    "    # Get the labels corresponding to the selected indices\n",
    "    selected_labels = labels[selected_indices]\n",
    "    \n",
    "    # Count class 0 samples in the selected subset\n",
    "    new_class_0 = np.sum(selected_labels == 0)\n",
    "    # Count class 1 samples in the selected subset\n",
    "    new_class_1 = np.sum(selected_labels == 1)\n",
    "    \n",
    "    # Print number of selected samples\n",
    "    print(f\"  Selected: {len(selected_indices):,} samples\")\n",
    "    # Print class 0 count and percentage in selected subset\n",
    "    print(f\"  Class 0 (no tumor): {new_class_0:,} ({new_class_0/len(selected_indices)*100:.1f}%)\")\n",
    "    # Print class 1 count and percentage in selected subset\n",
    "    print(f\"  Class 1 (tumor):    {new_class_1:,} ({new_class_1/len(selected_indices)*100:.1f}%)\")\n",
    "    \n",
    "    # Step 3: Extract only selected images (memory efficient!)\n",
    "    print(f\"\\nStep 3: Extracting {len(selected_indices):,} selected images...\")\n",
    "    \n",
    "    # Convert selected_indices to a set for O(1) lookup (quick)\n",
    "    selected_indices_set = set(selected_indices)\n",
    "    \n",
    "    try: # Try to find the actual h5 images file (manages both directory and file path)\n",
    "        # Call helper function to locate the h5 images file\n",
    "        actual_images_path = find_h5_file(images_h5_path)\n",
    "        # Print the resolved path to the images file\n",
    "        print(f\"  Found images H5: {actual_images_path}\")\n",
    "    # Catch any file not found errors\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"  ERROR: {e}\") # Print error message\n",
    "        raise # Re-raise the exception to stope execution\n",
    "    \n",
    "    # Open the h5 images file in read mode\n",
    "    with h5py.File(actual_images_path, 'r') as h5_file:\n",
    "        # Get the reference to the 'x' dataset containing images (without loading all the images into memory)\n",
    "        images_data = h5_file['x']\n",
    "        \n",
    "        # Extract only the selected indices\n",
    "        extracted_count = 0 # Initialize counter for successfully extracted images\n",
    "        # Loop through selected indices with enumeration for new sequential index\n",
    "        for new_idx, original_idx in enumerate(tqdm(selected_indices, desc=f\"Extracting {split_name}\")):\n",
    "            img_array = images_data[original_idx] # Read single image array from the h5 file at the original index\n",
    "            \n",
    "            # Convert the numpy array to a PIL Image object (specify RGB (color) mode)\n",
    "            img = Image.fromarray(img_array.astype('uint8'), 'RGB')\n",
    "            # Create the output path with zero-padded 5-digit filename\n",
    "            output_path = images_output_dir / f\"{new_idx:05d}.{save_format}\"\n",
    "            # Save the iamge to disk in specified format\n",
    "            img.save(output_path)\n",
    "            \n",
    "            # Save corresponding label\n",
    "            # Get the label value for this image from the selected labels\n",
    "            label_value = selected_labels[new_idx]\n",
    "            # Create the output path for the label text file\n",
    "            label_path = labels_output_dir / f\"{new_idx:05d}.txt\"\n",
    "            with open(label_path, 'w') as f: # Open the label file for writing\n",
    "                # Write the label as an integer string to the file\n",
    "                f.write(str(int(label_value)))\n",
    "            \n",
    "            extracted_count += 1 # Increment the extraction counter\n",
    "    \n",
    "    # Pritn success message with the extraction count\n",
    "    print(f\"\\n✓ Successfully extracted {extracted_count:,} images\")\n",
    "    # Print the split directory path\n",
    "    print(f\"  Split directory: {split_dir}\")\n",
    "    # Pritn the images subdirectory path\n",
    "    print(f\"  Images: {images_output_dir}\")\n",
    "    # Print the labels subdirectory path\n",
    "    print(f\"  Labels: {labels_output_dir}\")\n",
    "    \n",
    "    return selected_indices # Return the selected indices array for verification\n",
    "\n",
    "\n",
    "def extract_and_downsample_pcam(base_dir, output_dir, target_size=50000,\n",
    "                                 train_ratio=0.8, val_ratio=0.1, test_ratio=0.1,\n",
    "                                 random_seed=42, save_format='png'):\n",
    "    \"\"\"\n",
    "    Purpose:\n",
    "    --------\n",
    "    Extract and downsample entire PCAM dataset in one pass.\n",
    "    Memory efficient: never stores more than necessary images at once.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    base_dir : str or Path\n",
    "        Directory containing original PCAM .h5 files\n",
    "    output_dir : str or Path\n",
    "        Directory where downsampled extracted images will be saved\n",
    "    target_size : int\n",
    "        Total number of images in downsampled dataset\n",
    "    train_ratio, val_ratio, test_ratio : float\n",
    "        Ratios for splits\n",
    "    random_seed : int\n",
    "        Random seed for reproducibility (CRITICAL for reproducibility)\n",
    "    save_format : str\n",
    "        Image format to save ('png', 'jpg', etc.)\n",
    "    \"\"\"\n",
    "    # Convert the base directory string to a Path object\n",
    "    base_dir = Path(base_dir)\n",
    "    # Convert the output directory string to a Path object\n",
    "    output_dir = Path(output_dir)\n",
    "    # Create the output directory and all parent directories if they don't already exist\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Call helper function to calcualte how many images shoudl be in each split (train, val, test)\n",
    "    split_sizes = calculate_split_sizes(target_size, train_ratio, val_ratio, test_ratio)\n",
    "    \n",
    "    print(\"=\"*70) # Print header separator\n",
    "    # Pritn main title\n",
    "    print(\"PCAM EXTRACT & DOWNSAMPLE\")\n",
    "    print(\"=\"*70) # Print header separator\n",
    "    # Print the base directory path\n",
    "    print(f\"Base directory: {base_dir}\")\n",
    "    # Print the output directory path\n",
    "    print(f\"Output directory: {output_dir}\")\n",
    "    # Pritn target total dataset size\n",
    "    print(f\"Target total size: {target_size:,}\")\n",
    "    # Print random seed for reproducibility\n",
    "    print(f\"Random seed: {random_seed} (ensures reproducibility)\")\n",
    "    # Print section ehader for split size\n",
    "    print(f\"\\nTarget split sizes:\")\n",
    "    # Print train split size\n",
    "    print(f\"  Train: {split_sizes['train']:,} images\")\n",
    "    # Print validation split size\n",
    "    print(f\"  Valid: {split_sizes['valid']:,} images\")\n",
    "    # Print test split size\n",
    "    print(f\"  Test:  {split_sizes['test']:,} images\")\n",
    "    print(\"=\"*70) # Print header separator\n",
    "    \n",
    "    # Create list of split names to process\n",
    "    splits = ['train', 'valid', 'test']\n",
    "    # Initialize empty dictionary to store selected indices for each split\n",
    "    indices_log = {}\n",
    "    \n",
    "    for split in splits: # Loop through each split name\n",
    "        # Create path to the images h5 file for the current split\n",
    "        images_h5 = base_dir / f\"camelyonpatch_level_2_split_{split}_x.h5\"\n",
    "        # Create path to the labels h5 file for the current split\n",
    "        labels_h5 = base_dir / f\"camelyonpatch_level_2_split_{split}_y.h5\"\n",
    "        \n",
    "        # Check if the given images h5 file exists\n",
    "        if not images_h5.exists():\n",
    "            # Print warning and skip this split if the file isn't found\n",
    "            print(f\"\\nWARNING: {images_h5} not found, skipping {split}...\")\n",
    "            continue # Continue to next iteration of loop\n",
    "\n",
    "        # Check if the given labels h5 file exists\n",
    "        if not labels_h5.exists():\n",
    "            # Print warning and skip this split if the file isn't found\n",
    "            print(f\"\\nWARNING: {labels_h5} not found, skipping {split}...\")\n",
    "            continue # Continue to next iteration of loop\n",
    "        \n",
    "        # Call helper function to extract and downsample this split\n",
    "        selected_indices = extract_and_downsample_split(\n",
    "            images_h5_path=images_h5,\n",
    "            labels_h5_path=labels_h5,\n",
    "            output_dir=output_dir,\n",
    "            split_name=split,\n",
    "            n_samples=split_sizes[split],\n",
    "            random_seed=random_seed,\n",
    "            save_format=save_format\n",
    "        )\n",
    "        \n",
    "        # Store the selected indices in the log dictionary\n",
    "        indices_log[split] = selected_indices\n",
    "    \n",
    "    # Create path for file to save selected indices\n",
    "    indices_file = output_dir / \"selected_indices.npz\"\n",
    "    # Save all selected indices to compressed numpy file\n",
    "    np.savez(indices_file, **indices_log)\n",
    "    print(f\"\\n{'='*70}\") # Print separator\n",
    "    # Print confirmation of indices file save\n",
    "    print(f\"✓ Saved selected indices to {indices_file}\")\n",
    "    # Print note about file usage\n",
    "    print(\"  (This file can be used to verify reproducibility)\")\n",
    "    \n",
    "    # Print separator for completion section\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    # Print completion header\n",
    "    print(\"EXTRACTION & DOWNSAMPLING COMPLETE\")\n",
    "    print(\"=\"*70) # Print separator\n",
    "    # Print output directory path\n",
    "    print(f\"Downsampled dataset saved to: {output_dir}\")\n",
    "    # Calcualte and print total images extracted across all splits\n",
    "    print(f\"\\nTotal images extracted: {sum(len(indices_log[s]) for s in indices_log):,}\")\n",
    "    # Print section header for reproducibility instructions\n",
    "    print(\"\\nTo verify reproducibility:\")\n",
    "    # Print instruction with the random seed used\n",
    "    print(f\"  Run this script again with random_seed={random_seed}\")\n",
    "    # Pritn comparison instruction\n",
    "    print(\"  Compare the selected_indices.npz files\")\n",
    "    print(\"=\"*70) # Print final separator\n",
    "\n",
    "\n",
    "def find_h5_file(base_path):\n",
    "    \"\"\"\n",
    "    Purpose:\n",
    "    --------\n",
    "    Find the actual H5 file, handling both file and directory structures.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    base_path : Path\n",
    "        Base path that might be a file or directory containing the H5 file\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    Path : Path to the actual H5 file\n",
    "    \"\"\"\n",
    "    # Convert the base path to a Path object\n",
    "    base_path = Path(base_path)\n",
    "    \n",
    "    # Check if the path points to an existing file\n",
    "    if base_path.is_file():\n",
    "        # Return the path as-is since it's already a file\n",
    "        return base_path\n",
    "    \n",
    "    # Check if the path points to an existing directory\n",
    "    if base_path.is_dir():\n",
    "        # Construct path to file with same name inside directory\n",
    "        same_name_file = base_path / base_path.name\n",
    "        # Check if that same-name file exists\n",
    "        if same_name_file.is_file():\n",
    "            # Return the same-name file path\n",
    "            return same_name_file\n",
    "        \n",
    "        # Search for any h5 files in the directory\n",
    "        h5_files = list(base_path.glob(\"*.h5\"))\n",
    "        # Check if any h5 files were found\n",
    "        if h5_files:\n",
    "            # Return the first h5 file found\n",
    "            return h5_files[0]\n",
    "    \n",
    "    # Raise error if no h5 file could be found\n",
    "    raise FileNotFoundError(f\"Could not find H5 file at {base_path}\")\n",
    "\n",
    "\n",
    "def print_dataset_summary(base_dir):\n",
    "    \"\"\"\n",
    "    Purpose:\n",
    "    --------\n",
    "    Print summary statistics for all splits before extraction.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    base_dir : str or Path\n",
    "        Base directory containing all PCAM .h5 files\n",
    "    \"\"\"\n",
    "    # Convert base directory to Path object\n",
    "    base_dir = Path(base_dir)\n",
    "    # Create list of split names to analyze\n",
    "    splits = ['train', 'valid', 'test']\n",
    "    \n",
    "    print(\"=\"*70) # Print header separator\n",
    "    # Print summary title\n",
    "    print(\"PCAM Dataset Summary (Before Downsampling)\")\n",
    "    print(\"=\"*70) # Print header separator\n",
    "    \n",
    "    total_images = 0 # Initialize counter for total images across all splits\n",
    "    total_class_0 = 0 # Initialize counter for total class 0 samples\n",
    "    total_class_1 = 0 # Initialize counter for total class 1 samples\n",
    "    \n",
    "    # Loop through each split (train, val, test)\n",
    "    for split in splits:\n",
    "        # Construct path to labels h5 file for the current split\n",
    "        labels_h5_base = base_dir / f\"camelyonpatch_level_2_split_{split}_y.h5\"\n",
    "        \n",
    "        # Check if the base path exists\n",
    "        if not labels_h5_base.exists():\n",
    "            # Print message if the file isn't found\n",
    "            print(f\"{split.capitalize():10s}: File not found\")\n",
    "            continue # Continue to the next split (next loop iteration)\n",
    "        \n",
    "        try: # Try to find the actual h5 file\n",
    "            # Call helper function to locate the labels file\n",
    "            labels_h5 = find_h5_file(labels_h5_base)\n",
    "        # Catch file not found errors\n",
    "        except FileNotFoundError:\n",
    "            # Print error message if h5 file is not found in the directory\n",
    "            print(f\"{split.capitalize():10s}: H5 file not found in directory\")\n",
    "            continue # Continue to the next split (next loop iteration)\n",
    "        \n",
    "        # Open the labels h5 file in read mode\n",
    "        with h5py.File(labels_h5, 'r') as h5_file:\n",
    "            # Read all labels from the 'y' dataset\n",
    "            labels = h5_file['y'][:]\n",
    "            # Remove extra dimensions from labels array\n",
    "            labels = labels.squeeze()\n",
    "            \n",
    "            # Get total number of samples in this split\n",
    "            total = len(labels)\n",
    "            # Count class 0 samples in this split\n",
    "            class_0 = np.sum(labels == 0)\n",
    "            # Count class 1 samples in this split\n",
    "            class_1 = np.sum(labels == 1)\n",
    "            \n",
    "            # Print split name header with capitalization and padding\n",
    "            print(f\"\\n{split.capitalize():10s}:\")\n",
    "            # Print total images in this split\n",
    "            print(f\"  Total images: {total:,}\")\n",
    "            # Print class 0 count and percentage\n",
    "            print(f\"  Class 0 (no tumor): {class_0:,} ({class_0/total*100:.1f}%)\")\n",
    "            # Print class 1 count and percentage\n",
    "            print(f\"  Class 1 (tumor):    {class_1:,} ({class_1/total*100:.1f}%)\")\n",
    "            \n",
    "            total_images += total # Add current split's total to overall total\n",
    "            total_class_0 += class_0 # Add current split's class 0 count to overall class 0 count\n",
    "            total_class_1 += class_1 # Add current split's class 1 count to overall class 1 count\n",
    "    \n",
    "    print(f\"\\n{'='*70}\") # Print separator for summary section\n",
    "    print(f\"Total Dataset:\") # Print overall dataset header\n",
    "    # Print total images across all splits\n",
    "    print(f\"  Total images: {total_images:,}\")\n",
    "    # Print total class 0 samples and percentage\n",
    "    print(f\"  Class 0 (no tumor): {total_class_0:,} ({total_class_0/total_images*100:.1f}%)\")\n",
    "    # Print total class 1 samples and percentage\n",
    "    print(f\"  Class 1 (tumor):    {total_class_1:,} ({total_class_1/total_images*100:.1f}%)\")\n",
    "    print(\"=\"*70) # Print closing separator\n",
    "\n",
    "# Actual execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Define the base directory containing the PCAM h5 files\n",
    "    BASE_DIR = Path(r\"E:\\PCam\")\n",
    "    # Define the output directory for extracted images\n",
    "    OUTPUT_DIR = Path(r\"E:\\PCam_Extracted_100k\")\n",
    "    TARGET_SIZE = 100000 # Set target size for downsampled dataset\n",
    "    # Set rando mseed for reproducible sample (CRITICAL)\n",
    "    RANDOM_SEED = 42\n",
    "    \n",
    "    # Print original dataset summary header\n",
    "    print(\"Analyzing original dataset...\\n\")\n",
    "    # Call function to print summary of original dataset\n",
    "    print_dataset_summary(BASE_DIR)\n",
    "    \n",
    "    # Call function to calculate split sizes\n",
    "    split_sizes = calculate_split_sizes(TARGET_SIZE)\n",
    "    \n",
    "    # Print header for extraction plan\n",
    "    print(\"\\n\\nDataset Extraction Plan:\")\n",
    "    print(\"-\" * 70) # Print separator line\n",
    "    # Define original split sizes in dictionary\n",
    "    original_splits = {\n",
    "        'train': 262144,\n",
    "        'valid': 32768,\n",
    "        'test': 32768\n",
    "    }\n",
    "    \n",
    "    # Loop through each split to print extraction plan\n",
    "    for split in ['train', 'valid', 'test']:\n",
    "        # Get original size for this split\n",
    "        original = original_splits[split]\n",
    "        # Get new size for this split\n",
    "        new = split_sizes[split]\n",
    "        # Calcualte percentage of original that will be kept\n",
    "        kept_pct = (new / original) * 100\n",
    "        # Print extraction details for this split\n",
    "        print(f\"{split.capitalize():6s}: Extract {new:6,} of {original:,} \"\n",
    "              f\"({kept_pct:.1f}% of original)\")\n",
    "    print(\"-\" * 70) # Print separator line\n",
    "    \n",
    "    # Print message regarding actions\n",
    "    print(\"\\nThis will extract and downsample images directly from H5 files.\")\n",
    "    # Print note regarding memory\n",
    "    print(\"Only the selected subset will be saved to disk (memory efficient!).\")\n",
    "    # Prompt user for confirmation\n",
    "    response = input(\"\\nProceed? (yes/no): \")\n",
    "    \n",
    "    # Check if user confirmed with yes or y\n",
    "    if response.lower() in ['yes', 'y']:\n",
    "        # Run extraction and downsampling method\n",
    "        extract_and_downsample_pcam(\n",
    "            base_dir=BASE_DIR,\n",
    "            output_dir=OUTPUT_DIR,\n",
    "            target_size=TARGET_SIZE,\n",
    "            random_seed=RANDOM_SEED,\n",
    "            save_format='png'\n",
    "        )\n",
    "    else: # If user did not confirm\n",
    "        # Do nothing and print the cancellation message\n",
    "        print(\"Cancelled.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75058641",
   "metadata": {},
   "source": [
    "50K Images Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02a482c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing original dataset...\n",
      "\n",
      "======================================================================\n",
      "PCAM Dataset Summary (Before Downsampling)\n",
      "======================================================================\n",
      "\n",
      "Train     :\n",
      "  Total images: 262,144\n",
      "  Class 0 (no tumor): 131,072 (50.0%)\n",
      "  Class 1 (tumor):    131,072 (50.0%)\n",
      "\n",
      "Valid     :\n",
      "  Total images: 32,768\n",
      "  Class 0 (no tumor): 16,399 (50.0%)\n",
      "  Class 1 (tumor):    16,369 (50.0%)\n",
      "\n",
      "Test      :\n",
      "  Total images: 32,768\n",
      "  Class 0 (no tumor): 16,391 (50.0%)\n",
      "  Class 1 (tumor):    16,377 (50.0%)\n",
      "\n",
      "======================================================================\n",
      "Total Dataset:\n",
      "  Total images: 327,680\n",
      "  Class 0 (no tumor): 163,862 (50.0%)\n",
      "  Class 1 (tumor):    163,818 (50.0%)\n",
      "======================================================================\n",
      "\n",
      "\n",
      "Dataset Extraction Plan:\n",
      "----------------------------------------------------------------------\n",
      "Train : Extract 40,000 of 262,144 (15.3% of original)\n",
      "Valid : Extract  5,000 of 32,768 (15.3% of original)\n",
      "Test  : Extract  5,000 of 32,768 (15.3% of original)\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "This will extract and downsample images directly from H5 files.\n",
      "Only the selected subset will be saved to disk (memory efficient!).\n",
      "======================================================================\n",
      "PCAM EXTRACT & DOWNSAMPLE\n",
      "======================================================================\n",
      "Base directory: E:\\PCam\n",
      "Output directory: E:\\PCam_Extracted_50k\n",
      "Target total size: 50,000\n",
      "Random seed: 42 (ensures reproducibility)\n",
      "\n",
      "Target split sizes:\n",
      "  Train: 40,000 images\n",
      "  Valid: 5,000 images\n",
      "  Test:  5,000 images\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "Processing TRAIN split\n",
      "======================================================================\n",
      "Source: camelyonpatch_level_2_split_train_x.h5\n",
      "Target samples: 40000\n",
      "\n",
      "Step 1: Loading labels for stratification...\n",
      "  Found labels H5: E:\\PCam\\camelyonpatch_level_2_split_train_y.h5\\camelyonpatch_level_2_split_train_y.h5\n",
      "  Original size: 262,144\n",
      "  Class 0 (no tumor): 131,072 (50.0%)\n",
      "  Class 1 (tumor):    131,072 (50.0%)\n",
      "\n",
      "Step 2: Selecting stratified sample (reproducible with seed=42)...\n",
      "  Selected: 40,000 samples\n",
      "  Class 0 (no tumor): 20,000 (50.0%)\n",
      "  Class 1 (tumor):    20,000 (50.0%)\n",
      "\n",
      "Step 3: Extracting 40,000 selected images...\n",
      "  Found images H5: E:\\PCam\\camelyonpatch_level_2_split_train_x.h5\\camelyonpatch_level_2_split_train_x.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting train: 100%|██████████| 40000/40000 [22:27<00:00, 29.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Successfully extracted 40,000 images\n",
      "  Split directory: E:\\PCam_Extracted_50k\\train\n",
      "  Images: E:\\PCam_Extracted_50k\\train\\images\n",
      "  Labels: E:\\PCam_Extracted_50k\\train\\labels\n",
      "\n",
      "======================================================================\n",
      "Processing VALID split\n",
      "======================================================================\n",
      "Source: camelyonpatch_level_2_split_valid_x.h5\n",
      "Target samples: 5000\n",
      "\n",
      "Step 1: Loading labels for stratification...\n",
      "  Found labels H5: E:\\PCam\\camelyonpatch_level_2_split_valid_y.h5\\camelyonpatch_level_2_split_valid_y.h5\n",
      "  Original size: 32,768\n",
      "  Class 0 (no tumor): 16,399 (50.0%)\n",
      "  Class 1 (tumor):    16,369 (50.0%)\n",
      "\n",
      "Step 2: Selecting stratified sample (reproducible with seed=42)...\n",
      "  Selected: 5,000 samples\n",
      "  Class 0 (no tumor): 2,502 (50.0%)\n",
      "  Class 1 (tumor):    2,498 (50.0%)\n",
      "\n",
      "Step 3: Extracting 5,000 selected images...\n",
      "  Found images H5: E:\\PCam\\camelyonpatch_level_2_split_valid_x.h5\\camelyonpatch_level_2_split_valid_x.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting valid: 100%|██████████| 5000/5000 [01:13<00:00, 67.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Successfully extracted 5,000 images\n",
      "  Split directory: E:\\PCam_Extracted_50k\\valid\n",
      "  Images: E:\\PCam_Extracted_50k\\valid\\images\n",
      "  Labels: E:\\PCam_Extracted_50k\\valid\\labels\n",
      "\n",
      "======================================================================\n",
      "Processing TEST split\n",
      "======================================================================\n",
      "Source: camelyonpatch_level_2_split_test_x.h5\n",
      "Target samples: 5000\n",
      "\n",
      "Step 1: Loading labels for stratification...\n",
      "  Found labels H5: E:\\PCam\\camelyonpatch_level_2_split_test_y.h5\\camelyonpatch_level_2_split_test_y.h5\n",
      "  Original size: 32,768\n",
      "  Class 0 (no tumor): 16,391 (50.0%)\n",
      "  Class 1 (tumor):    16,377 (50.0%)\n",
      "\n",
      "Step 2: Selecting stratified sample (reproducible with seed=42)...\n",
      "  Selected: 5,000 samples\n",
      "  Class 0 (no tumor): 2,501 (50.0%)\n",
      "  Class 1 (tumor):    2,499 (50.0%)\n",
      "\n",
      "Step 3: Extracting 5,000 selected images...\n",
      "  Found images H5: E:\\PCam\\camelyonpatch_level_2_split_test_x.h5\\camelyonpatch_level_2_split_test_x.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting test: 100%|██████████| 5000/5000 [01:05<00:00, 75.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Successfully extracted 5,000 images\n",
      "  Split directory: E:\\PCam_Extracted_50k\\test\n",
      "  Images: E:\\PCam_Extracted_50k\\test\\images\n",
      "  Labels: E:\\PCam_Extracted_50k\\test\\labels\n",
      "\n",
      "======================================================================\n",
      "✓ Saved selected indices to E:\\PCam_Extracted_50k\\selected_indices.npz\n",
      "  (This file can be used to verify reproducibility)\n",
      "\n",
      "======================================================================\n",
      "EXTRACTION & DOWNSAMPLING COMPLETE\n",
      "======================================================================\n",
      "Downsampled dataset saved to: E:\\PCam_Extracted_50k\n",
      "\n",
      "Total images extracted: 50,000\n",
      "\n",
      "To verify reproducibility:\n",
      "  Run this script again with random_seed=42\n",
      "  Compare the selected_indices.npz files\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import h5py # Reading HDF5 (.h5) file format\n",
    "import numpy as np # Numerical operations and array manipulation\n",
    "from pathlib import Path # Cross-platform file path handling\n",
    "from PIL import Image # Image processing and saving\n",
    "from tqdm import tqdm # Displayinng progress bars during loops\n",
    "\n",
    "def calculate_split_sizes(total_target=50000, train_ratio=0.8, val_ratio=0.1, test_ratio=0.1):\n",
    "    \"\"\"\n",
    "    Purpose:\n",
    "    --------\n",
    "    Calculate how many images to keep in each split.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    total_target : int\n",
    "        Total number of images desired in downsampled dataset\n",
    "    train_ratio, val_ratio, test_ratio : float\n",
    "        Ratios for train/val/test split\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Number of images to keep for each split\n",
    "    \"\"\"\n",
    "    # Calculate number of training images by multiplying total by train ratio\n",
    "    train_size = int(total_target * train_ratio)\n",
    "    # Calculate number of validation images by multiplying total by validation ratio\n",
    "    val_size = int(total_target * val_ratio)\n",
    "    # Calculate number of test images by multiplying by test ratio\n",
    "    test_size = int(total_target * test_ratio)\n",
    "    \n",
    "    # Adjust to ensure exact total\n",
    "    # Calcualte sum of all three split sizes to check for rounding errors\n",
    "    actual_total = train_size + val_size + test_size\n",
    "    # Check if actual total differs from target due to integer rounding\n",
    "    if actual_total != total_target:\n",
    "        # Add the difference to the training set to reach the exact target total\n",
    "        train_size += (total_target - actual_total)\n",
    "    \n",
    "    # Return a dictionary mapping split names to their calculated sizes\n",
    "    return {\n",
    "        'train': train_size,\n",
    "        'valid': val_size,\n",
    "        'test': test_size\n",
    "    }\n",
    "\n",
    "\n",
    "def get_stratified_indices(labels, n_samples, random_seed=42):\n",
    "    \"\"\"\n",
    "    Purpose:\n",
    "    --------\n",
    "    Get stratified sample indices to maintain class balance.\n",
    "    Uses a fixed seed for reproducibility.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    labels : numpy.ndarray\n",
    "        Array of binary labels (0 or 1)\n",
    "    n_samples : int\n",
    "        Number of samples to select\n",
    "    random_seed : int\n",
    "        Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    selected_indices : numpy.ndarray\n",
    "        Sorted array of selected indices\n",
    "    \"\"\"\n",
    "    # Set random seed for reproducible sampling across runs\n",
    "    # Initializes the psuedo-random number generator (PRNG) with a specific starting point\n",
    "    np.random.seed(random_seed)\n",
    "    \n",
    "    # Get indices for each class\n",
    "    # Find all indices where the label is 0 (no tumor class)\n",
    "    class_0_indices = np.where(labels == 0)[0]\n",
    "    # Find all indices where the label is 1 (tumor class)\n",
    "    class_1_indices = np.where(labels == 1)[0]\n",
    "    \n",
    "    # Calculate how many of each class to keep (proportional)\n",
    "    # Count total number of class 0 samples\n",
    "    total_class_0 = len(class_0_indices)\n",
    "    # Count total number of class 1 samples\n",
    "    total_class_1 = len(class_1_indices)\n",
    "    # Calculate total number of sampels across both classes\n",
    "    total = total_class_0 + total_class_1\n",
    "    \n",
    "    # Calculate how many class 0 samples to keep, proportional to original distribution\n",
    "    n_class_0 = int(n_samples * (total_class_0 / total))\n",
    "    # Calculate class 1 samples as remainder to ensure exact n_samples total\n",
    "    n_class_1 = n_samples - n_class_0\n",
    "    \n",
    "    # Ensure we don't try to sample more than available\n",
    "    # Sample the smallest amount (either the number of class 0 claculated or the total number of class 0 images)\n",
    "    n_class_0 = min(n_class_0, total_class_0)\n",
    "    # Sample the smallest amount (either the number of class 1 claculated or the total number of class 01images)\n",
    "    n_class_1 = min(n_class_1, total_class_1)\n",
    "    \n",
    "    # Randomly sample from each class (but reproducibly due to the set seed)\n",
    "    # Randomly select n_class_0 indices from class 0 without replacement\n",
    "    selected_class_0 = np.random.choice(class_0_indices, size=n_class_0, replace=False)\n",
    "    # Randomly select n_class_1 indices from class 1 without replacement\n",
    "    selected_class_1 = np.random.choice(class_1_indices, size=n_class_1, replace=False)\n",
    "    \n",
    "    # Combine and sort indices\n",
    "    # Combine the selected indices from both classes into a single array\n",
    "    selected_indices = np.concatenate([selected_class_0, selected_class_1])\n",
    "    # Sort the indices in ascending order for sequential access\n",
    "    selected_indices = np.sort(selected_indices)\n",
    "    \n",
    "    return selected_indices # Return the sorted array of selected indices\n",
    "\n",
    "\n",
    "def extract_and_downsample_split(images_h5_path, labels_h5_path, output_dir, \n",
    "                                  split_name, n_samples, random_seed=42, \n",
    "                                  save_format='png'):\n",
    "    \"\"\"\n",
    "    Purpose:\n",
    "    --------\n",
    "    Extract and downsample a single split directly from H5 files.\n",
    "    Only extracts the selected subset, never storing all images.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    images_h5_path : str or Path\n",
    "        Path to input images .h5 file\n",
    "    labels_h5_path : str or Path\n",
    "        Path to input labels .h5 file\n",
    "    output_dir : str or Path\n",
    "        Base directory where extracted images will be saved\n",
    "    split_name : str\n",
    "        Name of the split ('train', 'valid', 'test')\n",
    "    n_samples : int\n",
    "        Number of samples to extract\n",
    "    random_seed : int\n",
    "        Random seed for reproducibility\n",
    "    save_format : str\n",
    "        Image format to save ('png', 'jpg', etc.)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    selected_indices : numpy.ndarray\n",
    "        Selected indices (for verification)\n",
    "    \"\"\"\n",
    "    # Ensure the path to the images is a Path object\n",
    "    images_h5_path = Path(images_h5_path)\n",
    "    # Ensure the path to the labels is a Path object\n",
    "    labels_h5_path = Path(labels_h5_path)\n",
    "    # Convert the output directory (string or Path) to a Path object\n",
    "    output_dir = Path(output_dir)\n",
    "    \n",
    "    # Create split-specific directories: output_dir/train, output_dir/val, output_dir/test\n",
    "    split_dir = output_dir / split_name\n",
    "    # Create path for images subdirectory within the split\n",
    "    images_output_dir = split_dir / \"images\"\n",
    "    # Create path for labels subdirectory within the split\n",
    "    labels_output_dir = split_dir / \"labels\"\n",
    "    # Create images directory and all parent directories if they don't already exist\n",
    "    images_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    # Create labels directory and all parent directories if they don't already exist\n",
    "    labels_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    print(f\"\\n{'='*70}\") # Print separator line\n",
    "    # Print header showing which split is being processed\n",
    "    print(f\"Processing {split_name.upper()} split\")\n",
    "    print(f\"{'='*70}\") # Print separator line\n",
    "    # Print the source h5 filename\n",
    "    print(f\"Source: {images_h5_path.name}\")\n",
    "    # Print the target number of samples to extract\n",
    "    print(f\"Target samples: {n_samples}\")\n",
    "    \n",
    "    # Step 1: Load labels to determine stratified sampling\n",
    "    print(\"\\nStep 1: Loading labels for stratification...\")\n",
    "\n",
    "    try: # Try to find actual h5 file (manages both file nad directory paths)\n",
    "        # Call helper function to find the h5 labels file\n",
    "        actual_labels_path = find_h5_file(labels_h5_path)\n",
    "        # Print the resolved path to the labels file\n",
    "        print(f\"  Found labels H5: {actual_labels_path}\")\n",
    "    # Catch any file not found errors\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"  ERROR: {e}\") # Print error message\n",
    "        raise # Re-raise the exception to stop execution\n",
    "    # Open the labels h5 file in read mode\n",
    "    with h5py.File(actual_labels_path, 'r') as h5_file:\n",
    "        # Read all labels from the 'y' dataset in the h5 file\n",
    "        labels = h5_file['y'][:]\n",
    "        # Remove extra dimensions (converting shape (n,1) to (n,))\n",
    "        labels = labels.squeeze()\n",
    "    \n",
    "    # Get the total number of labels (original dataset size)\n",
    "    original_size = len(labels)\n",
    "    # Count how many samples have label 0 (no tumor)\n",
    "    class_0_count = np.sum(labels == 0)\n",
    "    # Count how many samples have label 1 (tumor)\n",
    "    class_1_count = np.sum(labels == 1)\n",
    "    \n",
    "    # Print original dataset size with thousands separator\n",
    "    print(f\"  Original size: {original_size:,}\")\n",
    "    # Print class 0 count and percentage\n",
    "    print(f\"  Class 0 (no tumor): {class_0_count:,} ({class_0_count/original_size*100:.1f}%)\")\n",
    "    # Print class 1 count and percentage\n",
    "    print(f\"  Class 1 (tumor):    {class_1_count:,} ({class_1_count/original_size*100:.1f}%)\")\n",
    "    \n",
    "    # Step 2: Get stratified sample indices\n",
    "    # Print step 2 header with the random seed value used\n",
    "    print(\"\\nStep 2: Selecting stratified sample (reproducible with seed={})...\".format(random_seed))\n",
    "    # Call function to get stratified sample indices maintaining class balance\n",
    "    selected_indices = get_stratified_indices(labels, n_samples, random_seed)\n",
    "    # Get the labels corresponding to the selected indices\n",
    "    selected_labels = labels[selected_indices]\n",
    "    \n",
    "    # Count class 0 samples in the selected subset\n",
    "    new_class_0 = np.sum(selected_labels == 0)\n",
    "    # Count class 1 samples in the selected subset\n",
    "    new_class_1 = np.sum(selected_labels == 1)\n",
    "    \n",
    "    # Print number of selected samples\n",
    "    print(f\"  Selected: {len(selected_indices):,} samples\")\n",
    "    # Print class 0 count and percentage in selected subset\n",
    "    print(f\"  Class 0 (no tumor): {new_class_0:,} ({new_class_0/len(selected_indices)*100:.1f}%)\")\n",
    "    # Print class 1 count and percentage in selected subset\n",
    "    print(f\"  Class 1 (tumor):    {new_class_1:,} ({new_class_1/len(selected_indices)*100:.1f}%)\")\n",
    "    \n",
    "    # Step 3: Extract only selected images (memory efficient!)\n",
    "    print(f\"\\nStep 3: Extracting {len(selected_indices):,} selected images...\")\n",
    "    \n",
    "    # Convert selected_indices to a set for O(1) lookup (quick)\n",
    "    selected_indices_set = set(selected_indices)\n",
    "    \n",
    "    try: # Try to find the actual h5 images file (manages both directory and file path)\n",
    "        # Call helper function to locate the h5 images file\n",
    "        actual_images_path = find_h5_file(images_h5_path)\n",
    "        # Print the resolved path to the images file\n",
    "        print(f\"  Found images H5: {actual_images_path}\")\n",
    "    # Catch any file not found errors\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"  ERROR: {e}\") # Print error message\n",
    "        raise # Re-raise the exception to stope execution\n",
    "    \n",
    "    # Open the h5 images file in read mode\n",
    "    with h5py.File(actual_images_path, 'r') as h5_file:\n",
    "        # Get the reference to the 'x' dataset containing images (without loading all the images into memory)\n",
    "        images_data = h5_file['x']\n",
    "        \n",
    "        # Extract only the selected indices\n",
    "        extracted_count = 0 # Initialize counter for successfully extracted images\n",
    "        # Loop through selected indices with enumeration for new sequential index\n",
    "        for new_idx, original_idx in enumerate(tqdm(selected_indices, desc=f\"Extracting {split_name}\")):\n",
    "            img_array = images_data[original_idx] # Read single image array from the h5 file at the original index\n",
    "            \n",
    "            # Convert the numpy array to a PIL Image object (specify RGB (color) mode)\n",
    "            img = Image.fromarray(img_array.astype('uint8'), 'RGB')\n",
    "            # Create the output path with zero-padded 5-digit filename\n",
    "            output_path = images_output_dir / f\"{new_idx:05d}.{save_format}\"\n",
    "            # Save the iamge to disk in specified format\n",
    "            img.save(output_path)\n",
    "            \n",
    "            # Save corresponding label\n",
    "            # Get the label value for this image from the selected labels\n",
    "            label_value = selected_labels[new_idx]\n",
    "            # Create the output path for the label text file\n",
    "            label_path = labels_output_dir / f\"{new_idx:05d}.txt\"\n",
    "            with open(label_path, 'w') as f: # Open the label file for writing\n",
    "                # Write the label as an integer string to the file\n",
    "                f.write(str(int(label_value)))\n",
    "            \n",
    "            extracted_count += 1 # Increment the extraction counter\n",
    "    \n",
    "    # Pritn success message with the extraction count\n",
    "    print(f\"\\n✓ Successfully extracted {extracted_count:,} images\")\n",
    "    # Print the split directory path\n",
    "    print(f\"  Split directory: {split_dir}\")\n",
    "    # Pritn the images subdirectory path\n",
    "    print(f\"  Images: {images_output_dir}\")\n",
    "    # Print the labels subdirectory path\n",
    "    print(f\"  Labels: {labels_output_dir}\")\n",
    "    \n",
    "    return selected_indices # Return the selected indices array for verification\n",
    "\n",
    "\n",
    "def extract_and_downsample_pcam(base_dir, output_dir, target_size=50000,\n",
    "                                 train_ratio=0.8, val_ratio=0.1, test_ratio=0.1,\n",
    "                                 random_seed=42, save_format='png'):\n",
    "    \"\"\"\n",
    "    Purpose:\n",
    "    --------\n",
    "    Extract and downsample entire PCAM dataset in one pass.\n",
    "    Memory efficient: never stores more than necessary images at once.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    base_dir : str or Path\n",
    "        Directory containing original PCAM .h5 files\n",
    "    output_dir : str or Path\n",
    "        Directory where downsampled extracted images will be saved\n",
    "    target_size : int\n",
    "        Total number of images in downsampled dataset\n",
    "    train_ratio, val_ratio, test_ratio : float\n",
    "        Ratios for splits\n",
    "    random_seed : int\n",
    "        Random seed for reproducibility (CRITICAL for reproducibility)\n",
    "    save_format : str\n",
    "        Image format to save ('png', 'jpg', etc.)\n",
    "    \"\"\"\n",
    "    # Convert the base directory string to a Path object\n",
    "    base_dir = Path(base_dir)\n",
    "    # Convert the output directory string to a Path object\n",
    "    output_dir = Path(output_dir)\n",
    "    # Create the output directory and all parent directories if they don't already exist\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Call helper function to calcualte how many images shoudl be in each split (train, val, test)\n",
    "    split_sizes = calculate_split_sizes(target_size, train_ratio, val_ratio, test_ratio)\n",
    "    \n",
    "    print(\"=\"*70) # Print header separator\n",
    "    # Pritn main title\n",
    "    print(\"PCAM EXTRACT & DOWNSAMPLE\")\n",
    "    print(\"=\"*70) # Print header separator\n",
    "    # Print the base directory path\n",
    "    print(f\"Base directory: {base_dir}\")\n",
    "    # Print the output directory path\n",
    "    print(f\"Output directory: {output_dir}\")\n",
    "    # Pritn target total dataset size\n",
    "    print(f\"Target total size: {target_size:,}\")\n",
    "    # Print random seed for reproducibility\n",
    "    print(f\"Random seed: {random_seed} (ensures reproducibility)\")\n",
    "    # Print section ehader for split size\n",
    "    print(f\"\\nTarget split sizes:\")\n",
    "    # Print train split size\n",
    "    print(f\"  Train: {split_sizes['train']:,} images\")\n",
    "    # Print validation split size\n",
    "    print(f\"  Valid: {split_sizes['valid']:,} images\")\n",
    "    # Print test split size\n",
    "    print(f\"  Test:  {split_sizes['test']:,} images\")\n",
    "    print(\"=\"*70) # Print header separator\n",
    "    \n",
    "    # Create list of split names to process\n",
    "    splits = ['train', 'valid', 'test']\n",
    "    # Initialize empty dictionary to store selected indices for each split\n",
    "    indices_log = {}\n",
    "    \n",
    "    for split in splits: # Loop through each split name\n",
    "        # Create path to the images h5 file for the current split\n",
    "        images_h5 = base_dir / f\"camelyonpatch_level_2_split_{split}_x.h5\"\n",
    "        # Create path to the labels h5 file for the current split\n",
    "        labels_h5 = base_dir / f\"camelyonpatch_level_2_split_{split}_y.h5\"\n",
    "        \n",
    "        # Check if the given images h5 file exists\n",
    "        if not images_h5.exists():\n",
    "            # Print warning and skip this split if the file isn't found\n",
    "            print(f\"\\nWARNING: {images_h5} not found, skipping {split}...\")\n",
    "            continue # Continue to next iteration of loop\n",
    "\n",
    "        # Check if the given labels h5 file exists\n",
    "        if not labels_h5.exists():\n",
    "            # Print warning and skip this split if the file isn't found\n",
    "            print(f\"\\nWARNING: {labels_h5} not found, skipping {split}...\")\n",
    "            continue # Continue to next iteration of loop\n",
    "        \n",
    "        # Call helper function to extract and downsample this split\n",
    "        selected_indices = extract_and_downsample_split(\n",
    "            images_h5_path=images_h5,\n",
    "            labels_h5_path=labels_h5,\n",
    "            output_dir=output_dir,\n",
    "            split_name=split,\n",
    "            n_samples=split_sizes[split],\n",
    "            random_seed=random_seed,\n",
    "            save_format=save_format\n",
    "        )\n",
    "        \n",
    "        # Store the selected indices in the log dictionary\n",
    "        indices_log[split] = selected_indices\n",
    "    \n",
    "    # Create path for file to save selected indices\n",
    "    indices_file = output_dir / \"selected_indices.npz\"\n",
    "    # Save all selected indices to compressed numpy file\n",
    "    np.savez(indices_file, **indices_log)\n",
    "    print(f\"\\n{'='*70}\") # Print separator\n",
    "    # Print confirmation of indices file save\n",
    "    print(f\"✓ Saved selected indices to {indices_file}\")\n",
    "    # Print note about file usage\n",
    "    print(\"  (This file can be used to verify reproducibility)\")\n",
    "    \n",
    "    # Print separator for completion section\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    # Print completion header\n",
    "    print(\"EXTRACTION & DOWNSAMPLING COMPLETE\")\n",
    "    print(\"=\"*70) # Print separator\n",
    "    # Print output directory path\n",
    "    print(f\"Downsampled dataset saved to: {output_dir}\")\n",
    "    # Calcualte and print total images extracted across all splits\n",
    "    print(f\"\\nTotal images extracted: {sum(len(indices_log[s]) for s in indices_log):,}\")\n",
    "    # Print section header for reproducibility instructions\n",
    "    print(\"\\nTo verify reproducibility:\")\n",
    "    # Print instruction with the random seed used\n",
    "    print(f\"  Run this script again with random_seed={random_seed}\")\n",
    "    # Pritn comparison instruction\n",
    "    print(\"  Compare the selected_indices.npz files\")\n",
    "    print(\"=\"*70) # Print final separator\n",
    "\n",
    "\n",
    "def find_h5_file(base_path):\n",
    "    \"\"\"\n",
    "    Purpose:\n",
    "    --------\n",
    "    Find the actual H5 file, handling both file and directory structures.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    base_path : Path\n",
    "        Base path that might be a file or directory containing the H5 file\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    Path : Path to the actual H5 file\n",
    "    \"\"\"\n",
    "    # Convert the base path to a Path object\n",
    "    base_path = Path(base_path)\n",
    "    \n",
    "    # Check if the path points to an existing file\n",
    "    if base_path.is_file():\n",
    "        # Return the path as-is since it's already a file\n",
    "        return base_path\n",
    "    \n",
    "    # Check if the path points to an existing directory\n",
    "    if base_path.is_dir():\n",
    "        # Construct path to file with same name inside directory\n",
    "        same_name_file = base_path / base_path.name\n",
    "        # Check if that same-name file exists\n",
    "        if same_name_file.is_file():\n",
    "            # Return the same-name file path\n",
    "            return same_name_file\n",
    "        \n",
    "        # Search for any h5 files in the directory\n",
    "        h5_files = list(base_path.glob(\"*.h5\"))\n",
    "        # Check if any h5 files were found\n",
    "        if h5_files:\n",
    "            # Return the first h5 file found\n",
    "            return h5_files[0]\n",
    "    \n",
    "    # Raise error if no h5 file could be found\n",
    "    raise FileNotFoundError(f\"Could not find H5 file at {base_path}\")\n",
    "\n",
    "\n",
    "def print_dataset_summary(base_dir):\n",
    "    \"\"\"\n",
    "    Purpose:\n",
    "    --------\n",
    "    Print summary statistics for all splits before extraction.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    base_dir : str or Path\n",
    "        Base directory containing all PCAM .h5 files\n",
    "    \"\"\"\n",
    "    # Convert base directory to Path object\n",
    "    base_dir = Path(base_dir)\n",
    "    # Create list of split names to analyze\n",
    "    splits = ['train', 'valid', 'test']\n",
    "    \n",
    "    print(\"=\"*70) # Print header separator\n",
    "    # Print summary title\n",
    "    print(\"PCAM Dataset Summary (Before Downsampling)\")\n",
    "    print(\"=\"*70) # Print header separator\n",
    "    \n",
    "    total_images = 0 # Initialize counter for total images across all splits\n",
    "    total_class_0 = 0 # Initialize counter for total class 0 samples\n",
    "    total_class_1 = 0 # Initialize counter for total class 1 samples\n",
    "    \n",
    "    # Loop through each split (train, val, test)\n",
    "    for split in splits:\n",
    "        # Construct path to labels h5 file for the current split\n",
    "        labels_h5_base = base_dir / f\"camelyonpatch_level_2_split_{split}_y.h5\"\n",
    "        \n",
    "        # Check if the base path exists\n",
    "        if not labels_h5_base.exists():\n",
    "            # Print message if the file isn't found\n",
    "            print(f\"{split.capitalize():10s}: File not found\")\n",
    "            continue # Continue to the next split (next loop iteration)\n",
    "        \n",
    "        try: # Try to find the actual h5 file\n",
    "            # Call helper function to locate the labels file\n",
    "            labels_h5 = find_h5_file(labels_h5_base)\n",
    "        # Catch file not found errors\n",
    "        except FileNotFoundError:\n",
    "            # Print error message if h5 file is not found in the directory\n",
    "            print(f\"{split.capitalize():10s}: H5 file not found in directory\")\n",
    "            continue # Continue to the next split (next loop iteration)\n",
    "        \n",
    "        # Open the labels h5 file in read mode\n",
    "        with h5py.File(labels_h5, 'r') as h5_file:\n",
    "            # Read all labels from the 'y' dataset\n",
    "            labels = h5_file['y'][:]\n",
    "            # Remove extra dimensions from labels array\n",
    "            labels = labels.squeeze()\n",
    "            \n",
    "            # Get total number of samples in this split\n",
    "            total = len(labels)\n",
    "            # Count class 0 samples in this split\n",
    "            class_0 = np.sum(labels == 0)\n",
    "            # Count class 1 samples in this split\n",
    "            class_1 = np.sum(labels == 1)\n",
    "            \n",
    "            # Print split name header with capitalization and padding\n",
    "            print(f\"\\n{split.capitalize():10s}:\")\n",
    "            # Print total images in this split\n",
    "            print(f\"  Total images: {total:,}\")\n",
    "            # Print class 0 count and percentage\n",
    "            print(f\"  Class 0 (no tumor): {class_0:,} ({class_0/total*100:.1f}%)\")\n",
    "            # Print class 1 count and percentage\n",
    "            print(f\"  Class 1 (tumor):    {class_1:,} ({class_1/total*100:.1f}%)\")\n",
    "            \n",
    "            total_images += total # Add current split's total to overall total\n",
    "            total_class_0 += class_0 # Add current split's class 0 count to overall class 0 count\n",
    "            total_class_1 += class_1 # Add current split's class 1 count to overall class 1 count\n",
    "    \n",
    "    print(f\"\\n{'='*70}\") # Print separator for summary section\n",
    "    print(f\"Total Dataset:\") # Print overall dataset header\n",
    "    # Print total images across all splits\n",
    "    print(f\"  Total images: {total_images:,}\")\n",
    "    # Print total class 0 samples and percentage\n",
    "    print(f\"  Class 0 (no tumor): {total_class_0:,} ({total_class_0/total_images*100:.1f}%)\")\n",
    "    # Print total class 1 samples and percentage\n",
    "    print(f\"  Class 1 (tumor):    {total_class_1:,} ({total_class_1/total_images*100:.1f}%)\")\n",
    "    print(\"=\"*70) # Print closing separator\n",
    "\n",
    "# Actual execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Define the base directory containing the PCAM h5 files\n",
    "    BASE_DIR = Path(r\"E:\\PCam\")\n",
    "    # Define the output directory for extracted images\n",
    "    OUTPUT_DIR = Path(r\"E:\\PCam_Extracted_100k\")\n",
    "    TARGET_SIZE = 50000 # Set target size for downsampled dataset\n",
    "    # Set rando mseed for reproducible sample (CRITICAL)\n",
    "    RANDOM_SEED = 42\n",
    "    \n",
    "    # Print original dataset summary header\n",
    "    print(\"Analyzing original dataset...\\n\")\n",
    "    # Call function to print summary of original dataset\n",
    "    print_dataset_summary(BASE_DIR)\n",
    "    \n",
    "    # Call function to calculate split sizes\n",
    "    split_sizes = calculate_split_sizes(TARGET_SIZE)\n",
    "    \n",
    "    # Print header for extraction plan\n",
    "    print(\"\\n\\nDataset Extraction Plan:\")\n",
    "    print(\"-\" * 70) # Print separator line\n",
    "    # Define original split sizes in dictionary\n",
    "    original_splits = {\n",
    "        'train': 262144,\n",
    "        'valid': 32768,\n",
    "        'test': 32768\n",
    "    }\n",
    "    \n",
    "    # Loop through each split to print extraction plan\n",
    "    for split in ['train', 'valid', 'test']:\n",
    "        # Get original size for this split\n",
    "        original = original_splits[split]\n",
    "        # Get new size for this split\n",
    "        new = split_sizes[split]\n",
    "        # Calcualte percentage of original that will be kept\n",
    "        kept_pct = (new / original) * 100\n",
    "        # Print extraction details for this split\n",
    "        print(f\"{split.capitalize():6s}: Extract {new:6,} of {original:,} \"\n",
    "              f\"({kept_pct:.1f}% of original)\")\n",
    "    print(\"-\" * 70) # Print separator line\n",
    "    \n",
    "    # Print message regarding actions\n",
    "    print(\"\\nThis will extract and downsample images directly from H5 files.\")\n",
    "    # Print note regarding memory\n",
    "    print(\"Only the selected subset will be saved to disk (memory efficient!).\")\n",
    "    # Prompt user for confirmation\n",
    "    response = input(\"\\nProceed? (yes/no): \")\n",
    "    \n",
    "    # Check if user confirmed with yes or y\n",
    "    if response.lower() in ['yes', 'y']:\n",
    "        # Run extraction and downsampling method\n",
    "        extract_and_downsample_pcam(\n",
    "            base_dir=BASE_DIR,\n",
    "            output_dir=OUTPUT_DIR,\n",
    "            target_size=TARGET_SIZE,\n",
    "            random_seed=RANDOM_SEED,\n",
    "            save_format='png'\n",
    "        )\n",
    "    else: # If user did not confirm\n",
    "        # Do nothing and print the cancellation message\n",
    "        print(\"Cancelled.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
